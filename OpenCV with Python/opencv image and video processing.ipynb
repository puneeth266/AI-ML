{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8284b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 259, 3)\n",
      "(180, 320, 3)\n",
      "(600, 800, 3)\n",
      "(600, 800, 3)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "main_image = cv2.imread('car.jpeg')\n",
    "\n",
    "template = cv2.imread('carr1.jpeg')\n",
    "print(main_image.shape)\n",
    "print(template.shape)\n",
    "width = 800\n",
    "height = 600\n",
    "\n",
    "\n",
    "\n",
    "main_image = cv2.resize(main_image, (width, height))\n",
    "template = cv2.resize(template, (width, height))\n",
    "result = cv2.matchTemplate(main_image, template, cv2.TM_CCOEFF_NORMED)\n",
    "print(main_image.shape)\n",
    "print(template.shape)\n",
    "# Find the location of the matched area\n",
    "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "\n",
    "# Draw a rectangle around the matched area\n",
    "h, w, _ = template.shape\n",
    "cv2.rectangle(main_image, max_loc, (max_loc[0] + w, max_loc[1] + h), (0, 255, 0), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Matching Result', cv2.resize(main_image,(800,800)))\n",
    "cv2.imshow(\"Org Image\",template)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab993be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 480)\n",
      "(480, 480)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "im1=cv2.resize(cv2.imread(\"logo.jpeg\",cv2.IMREAD_GRAYSCALE),(480,480))\n",
    "im2=cv2.resize(cv2.imread(\"icon.png\",cv2.IMREAD_GRAYSCALE),(480,480))\n",
    "print(im1.shape)\n",
    "print(im2.shape)\n",
    "W,H=im1.shape[:2]\n",
    "#Read Images and Convert them to Grayscale\n",
    "match=cv2.matchTemplate(im1,im2,method=cv2.TM_CCOEFF_NORMED)\n",
    "threshold=126\n",
    "(x,y)=np.where(match>=threshold)\n",
    "boxes=[]\n",
    "for(a,b) in zip(x,y):\n",
    "    boxes.append((x,y,x+W,y+H))\n",
    "    \n",
    "for(a1,a2,a3,a4) in boxes:\n",
    "    cv2.rectangle(im1,(x1,y1),(x2,y2),(0,255,0),2)\n",
    "cv2.imshow(\"Final Output\",im1)\n",
    "cv2.imshow(\"Template Output 2\",im2)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88539639",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\arithm.cpp:650: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'cv::arithm_op'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     17\u001b[0m     expanded \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mpyrUp(gaussian_pyramid[i])\n\u001b[1;32m---> 18\u001b[0m     laplacian \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39msubtract(gaussian_pyramid[i \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], expanded)\n\u001b[0;32m     19\u001b[0m     laplacian_pyramid\u001b[38;5;241m.\u001b[39mappend(laplacian)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, level_image \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(laplacian_pyramid):\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\arithm.cpp:650: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'cv::arithm_op'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('icon.png')\n",
    "\n",
    "\n",
    "gaussian_pyramid = [image]\n",
    "for i in range(6):\n",
    "    image = cv2.pyrDown(image)\n",
    "    gaussian_pyramid.append(image)\n",
    "\n",
    "for i, level_image in enumerate(gaussian_pyramid):\n",
    "    cv2.imshow(f'Gaussian Pyramid Level {i}', level_image)\n",
    "# Construct the Laplacian pyramid from the Gaussian pyramid\n",
    "laplacian_pyramid = [gaussian_pyramid[5]]\n",
    "for i in range(5, 0, -1):\n",
    "    expanded = cv2.pyrUp(gaussian_pyramid[i])\n",
    "    laplacian = cv2.subtract(gaussian_pyramid[i - 1], expanded)\n",
    "    laplacian_pyramid.append(laplacian)\n",
    "\n",
    "for i, level_image in enumerate(laplacian_pyramid):\n",
    "    cv2.imshow(f'Laplacian Pyramid Level {i}', level_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "253b697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('logo.jpeg')\n",
    "\n",
    "gaussian_pyramid = [image]\n",
    "for i in range(6):\n",
    "    image = cv2.pyrDown(image)\n",
    "    gaussian_pyramid.append(image)\n",
    "\n",
    "for i, level_image in enumerate(gaussian_pyramid):\n",
    "    cv2.imshow(f'Gaussian Pyramid Level {i}', level_image)\n",
    "# Construct the Laplacian pyramid from the Gaussian pyramid\n",
    "laplacian_pyramid = [gaussian_pyramid[5]]\n",
    "for i in range(5, 0, -1):\n",
    "    expanded = cv2.pyrUp(gaussian_pyramid[i])\n",
    "    laplacian = cv2.subtract(cv2.resize(gaussian_pyramid[i],(480,480)),cv2.resize(expanded,(480,480)))\n",
    "    laplacian_pyramid.append(laplacian)\n",
    "\n",
    "for i, level_image in enumerate(laplacian_pyramid):\n",
    "    cv2.imshow(f'Laplacian Pyramid Level {i}', level_image)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a099977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load the image\n",
    "\n",
    "original_image = cv2.imread(\"icon.png\")\n",
    "\n",
    "# Convert the image to HSV\n",
    "hsv_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define different sizes\n",
    "sizes = [240, 360, 480, 720]\n",
    "\n",
    "# Create a directory to save the images\n",
    "output_directory = \"C:\\\\Users\\\\punee\\\\AI-ML\\\\Deep_learning_Exercises\\\\\"\n",
    "\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Save images with different sizes\n",
    "for size in sizes:\n",
    "    resized_image = cv2.resize(hsv_image, (size, size))\n",
    "    output_path = os.path.join(output_directory, f\"image{size}x{size}.jpg\")\n",
    "    cv2.imwrite(output_path, resized_image)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f879fe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load the image\n",
    "\n",
    "original_image = cv2.imread(\"icon.png\")\n",
    "\n",
    "# Apply a Gaussian blur filter\n",
    "blurred_image = cv2.GaussianBlur(original_image, (5, 5), 0)\n",
    "\n",
    "# Resize the image to a larger size\n",
    "resized_image = cv2.resize(blurred_image, (original_image.shape[1]*2, original_image.shape[0]*2))\n",
    "\n",
    "# Convert the resized image to HSV\n",
    "hsv_resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Define different sizes\n",
    "sizes = [240, 360, 480, 720]\n",
    "\n",
    "# Create a directory to save the images\n",
    "output_directory = \"output_images/\"\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "for i in sizes:\n",
    "    resized_image_for_output = cv2.resize(hsv_resized_image, (size, size))\n",
    "    output_path = os.path.join(output_directory, f\"image{i}x{i}.jpg\")\n",
    "    cv2.imwrite(output_path, resized_image_for_output)\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d746772a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(194, 259)\n",
      "15\n",
      "(300, 200)\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = \"car.jpeg\"\n",
    "original_image = cv2.imread(image, cv2.IMREAD_GRAYSCALE)\n",
    "print(original_image.shape)\n",
    "\n",
    "pixel_value = original_image[193, 258]\n",
    "print(pixel_value)\n",
    "\n",
    "\n",
    "width = 200\n",
    "height = 300\n",
    "\n",
    "\n",
    "\n",
    "original_image = cv2.resize(original_image, (width, height))\n",
    "print(original_image.shape)\n",
    "#resized_image1 = cv2.resize(image1, (width, height))\n",
    "# Threshold value\n",
    "threshold_value = 169\n",
    "\n",
    "# black image\n",
    "_, final_image = cv2.threshold(original_image, threshold_value, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "#final_image = np.zeros_like(original_image)\n",
    "\n",
    "# threshold split to black and white\n",
    "final_image[original_image < threshold_value] = 0\n",
    "final_image[original_image >= threshold_value] = 255\n",
    "\n",
    "# Blur \n",
    "blurred_image = cv2.blur(final_image, (5, 5), 0)\n",
    "\n",
    "# Save the image\n",
    "cv2.imwrite(\"res7.jpg\", blurred_image)\n",
    "\n",
    "print(\"d\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bce0d1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede9fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('num.jpeg', cv2.IMREAD_GRAYSCALE)\n",
    "cv2.imshow('original', image)\n",
    "blurred = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "\n",
    "circles = cv2.HoughCircles(\n",
    "    blurred, cv2.HOUGH_GRADIENT, dp=1, minDist=50, param1=200, param2=30, minRadius=10, maxRadius=50\n",
    ")\n",
    "if circles is not None:\n",
    "    circles = np.uint16(np.around(circles))\n",
    "    for circle in circles[0, :]:\n",
    "        cv2.circle(image, (circle[0], circle[1]), circle[2], (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "\n",
    "cv2.imshow('Hough Circles', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22ee55bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "image = cv2.imread('train.jpeg')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "edges = cv2.Canny(gray, 190, 20)\n",
    "lines = cv2.HoughLinesP(edges, 1, np.pi / 180, 150, maxLineGap=50)\n",
    "for line in lines:\n",
    "    x1,y1,x2,y2 = line[0]\n",
    "    cv2.line(image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "cv2.imshow(str((lines)), image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37947f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17eb8661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('group.jpeg')\n",
    "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "_, thresh = cv2.threshold(gray_image, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "# Perform morphological operations to clean the image\n",
    "kernel = np.ones((18, 18), np.uint8)\n",
    "opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations=2)\n",
    "\n",
    "# Identify the background area\n",
    "sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "# Perform distance transform\n",
    "dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "# Threshold the distance transform to obtain sure foreground\n",
    "_, sure_fg = cv2.threshold(dist_transform, 0.7 * dist_transform.max(), 255, 0)\n",
    "# Subtract sure foreground from sure background to obtain unknown region\n",
    "sure_fg = np.uint8(sure_fg)\n",
    "unknown = cv2.subtract(sure_bg, sure_fg)\n",
    "# Marker labeling for watershed algorithm\n",
    "_, markers = cv2.connectedComponents(sure_fg)\n",
    "markers = markers + 1\n",
    "markers[unknown == 255] = 0\n",
    "# Apply watershed algorithm\n",
    "cv2.watershed(image, markers)\n",
    "image[markers == -1] = [0, 0, 255]  # Mark watershed boundaries in red\n",
    "\n",
    "cv2.imshow('Segmented Image', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555e7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec0dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('group.jpeg')\n",
    "mask = np.zeros(image.shape[:2], np.uint8)\n",
    "# Define the rectangle for initial GrabCut algorithm\n",
    "rect = (50, 50, 450, 290)\n",
    "# Initialize the foreground and background models\n",
    "bgd_model = np.zeros((1, 65), np.float64)\n",
    "fgd_model = np.zeros((1, 65), np.float64)\n",
    "# Apply GrabCut algorithm\n",
    "cv2.grabCut(image, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)\n",
    "# Modify the mask to create a binary mask for foreground and background\n",
    "mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "# Multiply the original image with the binary mask to extract the foreground\n",
    "result = image * mask2[:, :, np.newaxis]\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Foreground Extraction', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34433386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b74ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6012d449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Read the image in grayscale\n",
    "image_path = \"car.jpeg\"\n",
    "original_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Threshold value\n",
    "threshold_value = 169\n",
    "\n",
    "# Find indices where pixel values are less than the threshold\n",
    "indices_below_threshold = original_image < threshold_value\n",
    "\n",
    "# Resize the image to increase pixel values\n",
    "resized_image = cv2.resize(original_image, (original_image.shape[1]*2, original_image.shape[0]*2))\n",
    "\n",
    "# Apply the threshold again to set pixel values to 255 where they are greater than or equal to the threshold\n",
    "resized_image[resized_image >= threshold_value] = 255\n",
    "\n",
    "# Save the resized image\n",
    "cv2.imwrite(\"resized_image2.jpg\", resized_image)\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d6e129",
   "metadata": {},
   "source": [
    "# object detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267fff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img1 = cv2.imread('icon.png', cv2.IMREAD_GRAYSCALE)\n",
    "img2 = cv2.imread('icon.jpeg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "#Create ORB detector\n",
    "orb = cv2.ORB_create()\n",
    "#get keypoints and descriptors\n",
    "keypoints1, descriptors1 = orb.detectAndCompute(img1, None)\n",
    "keypoints2, descriptors2 = orb.detectAndCompute(img2, None)\n",
    "\n",
    "# Create BFMatcher (Brute-Force Matcher) object\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "matches = bf.match(descriptors1, descriptors2)\n",
    "# Sort them based on distance\n",
    "matches = sorted(matches, key=lambda x: x.distance)\n",
    "img_matches = cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "cv2.imshow('Matches', img_matches)\n",
    "\n",
    "# Extract matched keypoints\n",
    "src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "homography, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "# Apply Homography to the template image corners to find the object\n",
    "h, w = img1.shape\n",
    "corners = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "transformed_corners = cv2.perspectiveTransform(corners, homography)\n",
    "img2_rectangle = cv2.polylines(img2, [np.int32(transformed_corners)], True, 255, 3, cv2.LINE_AA)\n",
    "\n",
    "cv2.imshow('Object Detection', img2_rectangle)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b479e215",
   "metadata": {},
   "source": [
    "# SIFT ALgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee89fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image = cv2.imread('group.jpeg', cv2.IMREAD_GRAYSCALE)\n",
    "# Create a SIFT object\n",
    "sift = cv2.SIFT_create()\n",
    "# Detect keypoints and compute descriptors\n",
    "keypoints, descriptors = sift.detectAndCompute(image, None)\n",
    "# Draw keypoints on the image\n",
    "image_with_keypoints = cv2.drawKeypoints(image, keypoints, None)\n",
    "cv2.imshow('Image with SIFT Keypoints', image_with_keypoints)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8292aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function detectAndCompute:\n",
      "\n",
      "detectAndCompute(...) method of cv2.SIFT instance\n",
      "    detectAndCompute(image, mask[, descriptors[, useProvidedKeypoints]]) -> keypoints, descriptors\n",
      "    .   Detects keypoints and computes the descriptors\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sift.detectAndCompute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07894044",
   "metadata": {},
   "source": [
    "# Harris algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6de2c3dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "NumPy boolean array indexing assignment cannot assign 2 input values to the 867 output values where the mask is true",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m corners\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mcornerHarris(flt,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m      7\u001b[0m corners\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mdilate(corners,\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m----> 8\u001b[0m img[corners \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.01\u001b[39m \u001b[38;5;241m*\u001b[39m corners\u001b[38;5;241m.\u001b[39mmax()]\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m255\u001b[39m]\n\u001b[0;32m      9\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinakl\u001b[39m\u001b[38;5;124m\"\u001b[39m,img)\n\u001b[0;32m     10\u001b[0m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: NumPy boolean array indexing assignment cannot assign 2 input values to the 867 output values where the mask is true"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "img=cv2.imread('face.jpeg',cv2.IMREAD_GRAYSCALE)\n",
    "flt=np.float32(img)\n",
    "\n",
    "corners=cv2.cornerHarris(flt,2,3,0.05)\n",
    "corners=cv2.dilate(corners,None)\n",
    "img[corners > 0.01 * corners.max()]=[0,255,0]\n",
    "cv2.imshow(\"Finakl\",img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2eff04",
   "metadata": {},
   "source": [
    "# Surf algo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac4272ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'xfeatures2d'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mface.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mIMREAD_GRAYSCALE)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create a SURF object\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m surf \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mxfeatures2d\u001b[38;5;241m.\u001b[39mSURF_create()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Detect keypoints and compute descriptors\u001b[39;00m\n\u001b[0;32m      7\u001b[0m keypoints, descriptors \u001b[38;5;241m=\u001b[39m surf\u001b[38;5;241m.\u001b[39mdetectAndCompute(image, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'xfeatures2d'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "image = cv2.imread('face.jpeg', cv2.IMREAD_GRAYSCALE)\n",
    "# Create a SURF object\n",
    "surf = cv2.xfeatures2d.SURF_create()\n",
    "# Detect keypoints and compute descriptors\n",
    "keypoints, descriptors = surf.detectAndCompute(image, None)\n",
    "# Draw keypoints on the image\n",
    "image_with_keypoints = cv2.drawKeypoints(image, keypoints, None)\n",
    "cv2.imshow('Image with SURF Keypoints', image_with_keypoints)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1850eed",
   "metadata": {},
   "source": [
    "# Shi-Tomasi Corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5e9c936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\punee\\AppData\\Local\\Temp\\ipykernel_15036\\233460930.py:8: DeprecationWarning: `np.int0` is a deprecated alias for `np.intp`.  (Deprecated NumPy 1.24)\n",
      "  corners = np.int0(corners)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "image = cv2.imread('face.jpeg', cv2.IMREAD_GRAYSCALE)\n",
    "# Detect Shi-Tomasi corners\n",
    "corners = cv2.goodFeaturesToTrack(image, maxCorners=25, qualityLevel=0.01, minDistance=10)\n",
    "# Convert corners to integers\n",
    "corners = np.int0(corners)\n",
    "# Draw circles around the detected corners\n",
    "image_with_corners = image.copy()\n",
    "for corner in corners:\n",
    "    x, y = corner.ravel()\n",
    "    cv2.circle(image_with_corners, (x, y), 6, 18, -1)\n",
    "\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Image with Shi-Tomasi Corners', image_with_corners)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7939f21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94246d9b",
   "metadata": {},
   "source": [
    "# Video processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44fe6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "vid=cv2.VideoCapture('hw.mp4')\n",
    "while vid.isOpened():\n",
    "    _,frame=vid.read()\n",
    "    frame=cv2.resize(frame,(640,640),interpolation=cv2.INTER_CUBIC)\n",
    "    cv2.imshow(\"Camera Capture\",frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86d996d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,os\n",
    "\n",
    "image_dir = r'C:\\Users\\punee\\AI-ML\\Deep_learning_Exercises'\n",
    "images=[img for img in os.listdir(image_dir) if img.endswith(\".png\" and \".jpeg\" and \".jpg\")]\n",
    "\n",
    "vid=cv2.VideoWriter(\"myVideo4.avi\",0,1,(480,480))\n",
    "for f in images:\n",
    "    vid.write(cv2.resize(cv2.imread(os.path.join(os.getcwd(),f)),(480,480)))\n",
    "cv2.destroyAllWindows()\n",
    "vid.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "92f47f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "image_dir = r'C:\\Users\\punee\\AI-ML\\Deep_learning_Exercises'\n",
    "images = [img for img in os.listdir(image_dir) if img.endswith((\".png\", \".jpeg\", \".jpg\"))]\n",
    "#images.sort()\n",
    "\n",
    "first_image = cv2.imread(os.path.join(image_dir, images[0]))\n",
    "height, width, layers = first_image.shape\n",
    "\n",
    "video=cv2.VideoWriter(\"myVideo6.mp4\",0,1,(480,480))\n",
    "\n",
    "for image_name in images:\n",
    "    \n",
    "    img = cv2.imread(os.path.join(image_dir, image_name))\n",
    "    img[:, :, 0] = 201\n",
    "    resized_img = cv2.resize(img, (480, 480))\n",
    "    #resized_img = cv2.resize(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), (480, 480))\n",
    "    video.write(resized_img)\n",
    "\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebcac4a",
   "metadata": {},
   "source": [
    "# frames in images in video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1513d8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import cv2,os\n",
    "cap=cv2.VideoCapture('myVideo6.mp4')\n",
    "count = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    count += 1\n",
    "\n",
    "    cv2.imshow(f'Video Frame {count}', frame)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78aacc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import cv2,os\n",
    "cap=cv2.VideoCapture('myVideo4.mp4')\n",
    "count = 0\n",
    "print(cv2.CAP_PROP_FPS)\n",
    "print(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "print(cv2.CAP_PROP_FRAME_COUNT)\n",
    "print(cv2.CAP_PROP_FORMAT)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    count += 1\n",
    "\n",
    "    #cv2.imshow(f'Video Frame {count}', frame)\n",
    "    #cv2.waitKey(0)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f3341fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "7\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import cv2,os\n",
    "cap=cv2.VideoCapture('myVideo4.mp4')\n",
    "count = 0\n",
    "print(cv2.CAP_PROP_FPS)\n",
    "print(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "print(cv2.CAP_PROP_FRAME_COUNT)\n",
    "print(cv2.CAP_PROP_FORMAT)\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    count += 1\n",
    "\n",
    "    #cv2.imshow(f'Video Frame {count}', frame)\n",
    "    #cv2.waitKey(0)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "287a8fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "4\n",
      "3\n",
      "7\n",
      "8\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "34.0\n",
      "0.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "1.0\n",
      "480.0\n",
      "480.0\n"
     ]
    }
   ],
   "source": [
    "import cv2,os\n",
    "cap=cv2.VideoCapture('myVideo4.mp4')\n",
    "count = 0\n",
    "print(cv2.CAP_PROP_FPS)\n",
    "print(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "print(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "print(cv2.CAP_PROP_FRAME_COUNT)\n",
    "print(cv2.CAP_PROP_FORMAT)\n",
    "\n",
    "print(cap.get(cv2.CAP_PROP_FPS))\n",
    "print(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "print(cap.get(cv2.CAP_PROP_FORMAT))\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    print(cap.get(cv2.CAP_PROP_FPS))\n",
    "    print(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    print(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    if not ret:\n",
    "        break\n",
    "    count += 1\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c1536c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "vid=cv2.VideoCapture('myVideo6.mp4')\n",
    "count = 0\n",
    "print(vid.get(cv2.CAP_PROP_FPS))\n",
    "print(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print(vid.get(cv2.CAP_PROP_AUDIO_TOTAL_CHANNELS))\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:break\n",
    "    if count >=10:\n",
    "        break\n",
    "    cv2.imshow(f\"Frame {count}\",frame)\n",
    "    cv2.waitKey(0)\n",
    "    count += 1\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "619de53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "vid=cv2.VideoCapture('myVideo6.mp4')\n",
    "count = 0\n",
    "print(vid.get(cv2.CAP_PROP_FPS))\n",
    "print(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print(vid.get(cv2.CAP_PROP_AUDIO_TOTAL_CHANNELS))\n",
    "\n",
    "while vid.isOpened():\n",
    "    \n",
    "    ret, frame = vid.read()\n",
    "    if not ret:break\n",
    "    if count >=10:\n",
    "        break\n",
    "    \n",
    "    \n",
    "    #cv2.imshow(f\"Frame {count}\",frame)\n",
    "    cv2.imwrite(f'prame {count}.png',frame)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    count += 1\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f3b31b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627724d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2,os\n",
    "\n",
    "image_dir = r'C:\\Users\\punee\\AI-ML\\Deep_learning_Exercises'\n",
    "images=[img for img in os.listdir(image_dir) if img.endswith(\".png\" and \".jpeg\" and \".jpg\")]\n",
    "\n",
    "vid=cv2.VideoWriter(\"myVideo4.avi\",0,1,(480,480))\n",
    "for f in images:\n",
    "    vid.write(cv2.resize(cv2.imread(os.path.join(os.getcwd(),f)),(480,480)))\n",
    "cv2.destroyAllWindows()\n",
    "vid.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea5c068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "vid=cv2.VideoCapture('myVideo6.mp4')\n",
    "count = 0\n",
    "print(vid.get(cv2.CAP_PROP_FPS))\n",
    "print(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print(vid.get(cv2.CAP_PROP_AUDIO_TOTAL_CHANNELS))\n",
    "\n",
    "vid1=cv2.VideoWriter(\"new.mp4\",0,1,(480,480))\n",
    "\n",
    "while vid.isOpened():\n",
    "    \n",
    "    ret, frame = vid.read()\n",
    "    if not ret:break\n",
    "    if count >=10:\n",
    "        break\n",
    "        \n",
    "    if count<=4:\n",
    "        guassian_blur_image = cv2.GaussianBlur(frame, (25,25),0)\n",
    "        vid1.write(cv2.resize(cv2.imread(os.path.join(os.getcwd(),guassian_blur_image)),(480,480)))\n",
    "    else:\n",
    "        vid1.write(cv2.resize(cv2.imread(os.path.join(os.getcwd(),frame)),(480,480)))\n",
    "            \n",
    "    \n",
    "    #cv2.imshow(f\"Frame {count}\",frame)\n",
    "    #cv2.imwrite(f'prame {count}.png',frame)\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "    count += 1\n",
    "vid.release()\n",
    "vid1.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13eb665",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "378ef152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "480.0\n",
      "480.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "vid = cv2.VideoCapture('myVideo6.mp4')\n",
    "count = 0\n",
    "\n",
    "print(vid.get(cv2.CAP_PROP_FPS))\n",
    "print(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "print(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "print(vid.get(cv2.CAP_PROP_AUDIO_TOTAL_CHANNELS))\n",
    "\n",
    "vid1=cv2.VideoWriter(\"new1.mp4\",0,1,(480,480))\n",
    "\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if count <= 4:\n",
    "        gaussian_blur_image = cv2.GaussianBlur(frame, (25, 25), 0)\n",
    "        vid1.write(gaussian_blur_image)\n",
    "    else:\n",
    "        vid1.write(frame)\n",
    "\n",
    "    count += 1\n",
    "    #if count >= 10:\n",
    "        #break\n",
    "vid.release()\n",
    "vid1.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485bc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "vid=cv2.VideoCapture('myVideo.avi')\n",
    "count = 0\n",
    "fps=vid.get(cv2.CAP_PROP_FPS)\n",
    "height=(vid.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "width=(vid.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "start=int(input(\"Enter the Time from where you want to trim\"))\n",
    "finall=int(input(\"Enter the Time till where you want to trim\"))\n",
    "vid2=cv2.VideoWriter(\"NewVideo.avi\",0,fps,(height,width))\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:break\n",
    "    if count >=start and count< finall:\n",
    "        vid2.write(frame)\n",
    "    count += 1\n",
    "vid.release()\n",
    "vid2.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b33329",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85674197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22d42913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480.0\n",
      "480.0\n",
      "hurray\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "vidx=cv2.VideoCapture('myVideo6.mp4')\n",
    "vidy=cv2.VideoCapture('myVideo7.mp4')\n",
    "\n",
    "count = 0\n",
    "fpsx=(vidx.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "heightx=(vidx.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "widthx=(vidx.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "heighty=(vidy.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "widthy=(vidy.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "\n",
    "\n",
    "height=max(heightx, heighty)\n",
    "width=max(widthx, widthy)\n",
    "print(height)\n",
    "print(width)\n",
    "\n",
    "\n",
    "#start=int(input(\"Enter the Time from where you want to trim\"))\n",
    "#finall=int(input(\"Enter the Time till where you want to trim\"))\n",
    "\n",
    "vid2=cv2.VideoWriter(\"NewVideo11.mp4\",0,1,(int(height),int(width)))\n",
    "#vid2=cv2.VideoWriter('newn2.mp4',0,1,(480,480))\n",
    "\n",
    "\n",
    "while vidx.isOpened():\n",
    "    ret, frame = vidx.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    vid2.write(frame)\n",
    "\n",
    "#merge\n",
    "while vidy.isOpened():\n",
    "    ret, frame = vidy.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    vid2.write(frame)\n",
    "    \n",
    "vidx.release()\n",
    "vidy.release()\n",
    "vid2.release()\n",
    "#vidy.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hurray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a6f07",
   "metadata": {},
   "source": [
    "# canny video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8c557c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hurray\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2\n",
    "vid1=cv2.VideoCapture('myVideo6.mp4')\n",
    "count = 0\n",
    "fps=(vid1.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "\n",
    "#vid2=cv2.VideoWriter(\"NewVideo11.mp4\",0,1,(int(height),int(width)))\n",
    "vid2=cv2.VideoWriter('canny.mp4',0,1,(480,480))\n",
    "\n",
    "\n",
    "while vid1.isOpened():\n",
    "    ret, frame = vid1.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    edges = cv2.Canny(frame, 50, 150)\n",
    "\n",
    "    vid2.write(edges)\n",
    "\n",
    "    \n",
    "vid1.release()\n",
    "vid2.release()\n",
    "#vidy.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hurray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e64187",
   "metadata": {},
   "source": [
    "# histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4d6761dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hurray\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "cap = cv2.VideoCapture('myVideo6.mp4')\n",
    "count = 0\n",
    "\n",
    "bins = 256 \n",
    "range_ = (0, 256)  \n",
    "\n",
    "vid2=cv2.VideoWriter('hist2.mp4',0,1,(480,480))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    hist = cv2.calcHist([frame], [0], None, [bins], range_)\n",
    "\n",
    "    output=cv2.normalize(hist, hist, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX)\n",
    "    #count += 1\n",
    "    vid2.write(output)\n",
    "\n",
    "    #cv2.imshow(f'Original Frame {count}', frame)\n",
    "    #cv2.imshow(f'Histogram Frame {count}', hist)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "    \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hurray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1c18c4",
   "metadata": {},
   "source": [
    "# face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "abc50ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "face = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "image_path = 'group.jpeg'\n",
    "img = cv2.imread(image_path)\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "faces = face.detectMultiScale(gray_img, scaleFactor=1.3, minNeighbors=5)\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "cv2.imshow('Detected Faces', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb98ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "face = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "image_path = 'group.jpeg'\n",
    "img = cv2.imread(image_path)\n",
    "gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "faces = face.detectMultiScale(gray_img, scaleFactor=1.3, minNeighbors=5)\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "cv2.imshow('Detected Faces', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43885602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccb147c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hurray\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "vid1=cv2.VideoCapture('myVideo6.mp4')\n",
    "count = 0\n",
    "fps=(vid1.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "\n",
    "#vid2=cv2.VideoWriter(\"NewVideo11.mp4\",0,1,(int(height),int(width)))\n",
    "#vid2=cv2.VideoWriter('face1.mp4',0,1,(480,480))\n",
    "output=\"detected\"\n",
    "face = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while vid1.isOpened():\n",
    "    ret, frame = vid1.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        face_img = frame[y:y+h, x:x+w]\n",
    "        #cv2.imshow(f'{output}/face_{count}.jpg', face_img)\n",
    "        cv2.imwrite(f'face_{count}.jpg', face_img)\n",
    "        count += 1\n",
    "          \n",
    "vid1.release()\n",
    "#vid2.release()\n",
    "#vidy.release()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hurray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d9f2e6",
   "metadata": {},
   "source": [
    "# saving as individual faces in images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1c86422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hurray\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import cv2\n",
    "vid1=cv2.VideoCapture('myVideo6.mp4')\n",
    "count = 0\n",
    "fps=(vid1.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "\n",
    "#vid2=cv2.VideoWriter(\"NewVideo11.mp4\",0,1,(int(height),int(width)))\n",
    "#vid2=cv2.VideoWriter('face1.mp4',0,1,(480,480))\n",
    "#output=\"detected\"\n",
    "face = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while vid1.isOpened():\n",
    "    ret, frame = vid1.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face.detectMultiScale(gray, scaleFactor=3.9, minNeighbors=5)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        face_=cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "    cv2.imshow(f'new_face1{count}.jpg', face_)\n",
    "    #cv2.imwrite(f'new_face{count}.jpg', face_)\n",
    "\n",
    "    count += 1\n",
    "       \n",
    "vid1.release()\n",
    "#vid2.release()\n",
    "#vidy.release()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hurray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89089f21",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# saving the faces into video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "93d497c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hurray\n"
     ]
    }
   ],
   "source": [
    "###################################################### should do\n",
    "\n",
    "import cv2\n",
    "vid1=cv2.VideoCapture('NewVideo11.mp4')\n",
    "count = 0\n",
    "fps=(vid1.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "\n",
    "#vid2=cv2.VideoWriter(\"NewVideo11.mp4\",0,1,(int(height),int(width)))\n",
    "vid2=cv2.VideoWriter('face_v1.mp4',0,1,(480,480))\n",
    "#output=\"detected\"\n",
    "face = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while vid1.isOpened():\n",
    "    ret, frame = vid1.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face.detectMultiScale(gray, scaleFactor=3.9, minNeighbors=5)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        face_=cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    vid2.write(face_)\n",
    "    #cv2.imshow(f'new_face1{count}.jpg', face_)\n",
    "    #cv2.imwrite(f'new_face{count}.jpg', face_)\n",
    "\n",
    "    count += 1\n",
    "       \n",
    "vid1.release()\n",
    "vid2.release()\n",
    "#vidy.release()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hurray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62921a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "main_image = cv2.imread('car.jpeg')\n",
    "\n",
    "template = cv2.imread('car2.jpeg')\n",
    "result = cv2.matchTemplate(main_image, template, cv2.TM_CCOEFF_NORMED)\n",
    "\n",
    "# Find the location of the matched area\n",
    "min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "\n",
    "# Draw a rectangle around the matched area\n",
    "h, w, _ = template.shape\n",
    "cv2.rectangle(main_image, max_loc, (max_loc[0] + w, max_loc[1] + h), (0, 255, 0), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Matching Result', cv2.resize(main_image,(800,800)))\n",
    "cv2.imshow(\"Org Image\",template)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20237f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4679caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################### should do\n",
    "\n",
    "import cv2\n",
    "vid1=cv2.VideoCapture('myVideo6.mp4')\n",
    "count = 0\n",
    "fps=(vid1.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "face = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while vid1.isOpened():\n",
    "    ret, frame = vid1.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    faces = face.detectMultiScale(gray, scaleFactor=3.9, minNeighbors=5)\n",
    "    \n",
    "    for (x, y, w, h) in faces:\n",
    "        face_=cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "        \n",
    "    #cv2.imshow(f'new_face1{count}.jpg', face_)\n",
    "    #cv2.imwrite(f'new_face{count}.jpg', face_)\n",
    "\n",
    "    count += 1\n",
    "       \n",
    "vid1.release()\n",
    "#vid2.release()\n",
    "#vidy.release()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hurray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b845b4a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d0d22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0c24b1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched\n",
      "matched\n",
      "hurray\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Template matching code\n",
    "def template_matching(main_image, template):\n",
    "    result = cv2.matchTemplate(main_image, template, cv2.TM_CCOEFF_NORMED)\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "    h, w, _ = template.shape\n",
    "\n",
    "    if max_val > threshold:  # You can set a threshold to control the matches\n",
    "        # Draw a rectangle around the matched area\n",
    "        cv2.rectangle(main_image, max_loc, (max_loc[0] + w, max_loc[1] + h), (0, 255, 0), 2)\n",
    "        return True, main_image\n",
    "    else:\n",
    "        return False, main_image\n",
    "\n",
    "# Main video capture\n",
    "vid1 = cv2.VideoCapture('myVideo6.mp4')\n",
    "count = 0\n",
    "fps = vid1.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "car_template = cv2.imread('face_0.jpg')\n",
    "threshold = 0.7  # Adjust the threshold based on your needs\n",
    "\n",
    "while vid1.isOpened():\n",
    "    ret, frame = vid1.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=3.9, minNeighbors=5)\n",
    "    \n",
    "    # Iterate through detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        # Apply template matching to the face ROI\n",
    "        matched, result_image = template_matching(face_roi, car_template)\n",
    "        print(\"matched\")\n",
    "        if matched:\n",
    "            cv2.imshow(f'Matched Car {count}.jpg', cv2.resize(result_image, (800, 800)))\n",
    "        count += 1\n",
    "\n",
    "    # Display the frame with detected faces\n",
    "    cv2.imshow('Detected Faces', cv2.resize(frame, (800, 800)))\n",
    "\n",
    "    #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #break\n",
    "\n",
    "vid1.release()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hurray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9ffb8f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hurray\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e5aa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def template_matching(main_image, template):\n",
    "    result = cv2.matchTemplate(main_image, template, cv2.TM_CCOEFF_NORMED)\n",
    "    min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n",
    "    h, w, _ = template.shape\n",
    "\n",
    "    if max_val > threshold:  \n",
    "        cv2.rectangle(main_image, max_loc, (max_loc[0] + w, max_loc[1] + h), (0, 255, 0), 2)\n",
    "        return True, main_image\n",
    "    else:\n",
    "        return False, main_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35255153",
   "metadata": {},
   "source": [
    "# matching a face inside video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5a57db9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "hurray\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "face_template = cv2.imread('face_0.jpg')\n",
    "threshold = 0.6  # Adjust the threshold based on your needs\n",
    "\n",
    "vid = cv2.VideoCapture('NewVideo11.mp4')\n",
    "count = 0\n",
    "\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    matched, result_image = template_matching(frame, face_template)\n",
    "\n",
    "    if matched:\n",
    "        print(matched)\n",
    "        cv2.imshow(f'Matched Face {count}.jpg', cv2.resize(result_image, (800, 800)))\n",
    "        count += 1\n",
    "\n",
    "vid.release()\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hurray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f61f4ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(x) for x in range(0,11,2)]\n",
    "for(a=10;a<=10;a++) \n",
    "\n",
    "#{print(x) for x in range(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "791857ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "vid=cv2.VideoCapture('myVideo4.mp4')\n",
    "#lk=dict()\n",
    "mask=np.random.randint(0,255,(100,4))\n",
    "ret1, frame1 = vid.read()\n",
    "gray1=cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n",
    "points1=cv2.goodFeaturesToTrack(gray1,mask=None,maxCorners=100,qualityLevel=0.5,minDistance=10,blockSize=10)\n",
    "\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:break\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    points2,status,err=cv2.calcOpticalFlowPyrLK(gray1,gray,points1,None,winSize=(15,15),maxLevel=2,criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT,10,0.03))\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d961ee9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter the starting frame: 6\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "d\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "vid=cv2.VideoCapture('myVideo6.mp4')\n",
    "#lk=dict()\n",
    "frameS=int(input(\"enter the starting frame: \"))\n",
    "vid.set(cv2.CAP_PROP_POS_FRAMES, frameS)\n",
    "ret1, frame1 = vid.read()\n",
    "gray1=cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n",
    "points1=cv2.goodFeaturesToTrack(gray1,mask=None,maxCorners=100,qualityLevel=0.5,minDistance=10,blockSize=10)\n",
    "mask2=np.zeros_like(frame1)\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:break\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    points2,status,err=cv2.calcOpticalFlowPyrLK(gray1,gray,points1,None,winSize=(15,15),maxLevel=2,criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT,10,0.03))\n",
    "    newpts=points1[status==1]\n",
    "    oldpts=points1[status==1]\n",
    "    for x,(new,old) in enumerate(zip(newpts,oldpts)):\n",
    "        a,b=new.ravel()#2D->1D Array\n",
    "        c,d=old.ravel()\n",
    "        mask2=cv2.line(mask2,(int(a),int(b)),(int(c),int(d)),(0,255,255),4)\n",
    "        frame=cv2.circle(frame,(int(a),int(b)),5,(0,255,0),-1)\n",
    "    print(frame.shape)\n",
    "    print(mask2.shape)\n",
    "    image=cv2.add(frame,mask2)\n",
    "    cv2.imshow(\"Frame\",image)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "vid.release\n",
    "print(\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a1a4e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23a1f9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "(480, 480, 3)\n",
      "hi\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "vid=cv2.VideoCapture('myVideo6.mp4')\n",
    "#lk=dict()\n",
    "ret1, frame1 = vid.read()\n",
    "gray1=cv2.cvtColor(frame1,cv2.COLOR_BGR2GRAY)\n",
    "points1=cv2.goodFeaturesToTrack(gray1,mask=None,maxCorners=100,qualityLevel=0.5,minDistance=10,blockSize=10)\n",
    "#print(points1)\n",
    "\n",
    "mask2=np.zeros_like(frame1)\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:break\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    points2,status,err=cv2.calcOpticalFlowPyrLK(gray1,gray,points1,None,winSize=(15,15),maxLevel=2,criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT,10,0.03))\n",
    "    newpts=points1[status==1]\n",
    "    oldpts=points1[status==1]\n",
    "    for x,(new,old) in enumerate(zip(newpts,oldpts)):\n",
    "        a,b=new.ravel()#2D->1D Array\n",
    "        c,d=old.ravel()\n",
    "        mask2=cv2.line(mask2,(int(a),int(b)),(int(c),int(d)),(0,255,255),4)\n",
    "        frame=cv2.circle(frame,(int(a),int(b)),5,(0,255,0),-1)\n",
    "    print(frame.shape)\n",
    "    print(mask2.shape)\n",
    "    image=cv2.add(frame,mask2)\n",
    "    cv2.imshow(\"Frame\",image)\n",
    "    #cv2.waitK\n",
    "cv2.waitKey(0)\n",
    "gray1=gray.copy()\n",
    "points1=newpts.reshape(-1,1,2)\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hi\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdd367f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "vid=cv2.VideoCapture('myVideo6.mp4')\n",
    "x,y,h,w=200,200,100,100\n",
    "roi=(x,y,w,h)\n",
    "criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT,10,1)\n",
    "ret1, frame1 = vid.read()\n",
    "frame_roi=frame1[y:y+h,x:x+w]\n",
    "hsv=cv2.cvtColor(frame_roi,cv2.COLOR_BGR2HSV)\n",
    "hist=cv2.calcHist([hsv],[0],None,[180],[0,180])\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:break\n",
    "    hsv_frame=cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "    dst=cv2.calcBackProject([hsv_frame],[0],hist,[0,180],1)\n",
    "    outp,rei=cv2.meanShift(dst,roi,criteria)\n",
    "    x,y,w,h=roi\n",
    "    result=cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,255),2)\n",
    "    cv2.imshow(\"Meanshift\",result)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "vid.release()\n",
    "vid=cv2.VideoCapture('myVideo6.mp4')\n",
    "x,y,h,w=200,200,100,100\n",
    "roi=(x,y,w,h)\n",
    "criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT,10,1)\n",
    "ret1, frame1 = vid.read()\n",
    "frame_roi=frame1[y:y+h,x:x+w]\n",
    "hsv=cv2.cvtColor(frame_roi,cv2.COLOR_BGR2HSV)\n",
    "hist=cv2.calcHist([hsv],[0],None,[180],[0,180])\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:break\n",
    "    hsv_frame=cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "    dst=cv2.calcBackProject([hsv_frame],[0],hist,[0,180],1)\n",
    "    outp,rei=cv2.CamShift(dst,roi,criteria)\n",
    "    points=cv2.boxPoints(outp).astype(int)\n",
    "    result=cv2.polylines(frame,[points],True,(255,0,0),2)\n",
    "    cv2.imshow(\"Camshift\",result)\n",
    "    cv2.waitKey(0)\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f90cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "vid=cv2.VideoCapture('myVideo6.mp4')\n",
    "\n",
    "bg_subtractor = cv2.createBackgroundSubtractorKNN(history=500, dist2Threshold=16, detectShadows=False)\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:break\n",
    "    fg_mask = bg_subtractor.apply(frame)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)\n",
    "    cv2.imshow('Original Frame', frame)\n",
    "    cv2.imshow('Foreground Mask', fg_mask)\n",
    "    cv2.waitKey(0)\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "vid=cv2.VideoCapture('myVideo7.mp4')\n",
    "\n",
    "bg_subtractor = cv2.createBackgroundSubtractorKNN(history=500, dist2Threshold=16, detectShadows=False)\n",
    "count=0\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:break\n",
    "    count+=1\n",
    "    fg_mask = bg_subtractor.apply(frame)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n",
    "    fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_OPEN, kernel)\n",
    "    cv2.imshow('Original Frame', frame)\n",
    "    cv2.imshow('Foreground Mask', fg_mask)\n",
    "    if count > 50:\n",
    "        break\n",
    "    cv2.waitKey(0)\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e96f2",
   "metadata": {},
   "source": [
    "# orb detector for videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e15a38cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\matmul.dispatch.cpp:550: error: (-215:Assertion failed) scn + 1 == m.cols in function 'cv::perspectiveTransform'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 41\u001b[0m\n\u001b[0;32m     39\u001b[0m h, w \u001b[38;5;241m=\u001b[39m gray1\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     40\u001b[0m corners \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m, h \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], [w \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, h \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m], [w \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m transformed_corners \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mperspectiveTransform(corners, homography)\n\u001b[0;32m     42\u001b[0m img2_rectangle \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mpolylines(img2, [np\u001b[38;5;241m.\u001b[39mint32(transformed_corners)], \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m3\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mLINE_AA)\n\u001b[0;32m     44\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mframe_sift_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.1) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\core\\src\\matmul.dispatch.cpp:550: error: (-215:Assertion failed) scn + 1 == m.cols in function 'cv::perspectiveTransform'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "orb = cv2.ORB_create()\n",
    "\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "vid = cv2.VideoCapture('myVideo4.mp4')\n",
    "\n",
    "\n",
    "ret, img1 = vid.read()\n",
    "gray1 = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
    "keypoints1, descriptors1 = orb.detectAndCompute(gray1, None)\n",
    "\n",
    "output_ = \"C:\\\\Users\\\\punee\\\\AI-ML\\\\Deep_learning_Exercises\\\\output_orb\"\n",
    "\n",
    "while vid.isOpened():\n",
    "    \n",
    "    ret, img2 = vid.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray2 = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
    "    keypoints2, descriptors2 = orb.detectAndCompute(gray2, None)\n",
    "\n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "\n",
    "    img_matches = cv2.drawMatches(img1, keypoints1, img2, keypoints2, matches[:10], None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "    cv2.imshow('Matches', img_matches)\n",
    "\n",
    "    src_pts = np.float32([keypoints1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([keypoints2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    homography, _ = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "\n",
    "    h, w = gray1.shape\n",
    "    corners = np.float32([[0, 0], [0, h - 1], [w - 1, h - 1], [w - 1, 0]]).reshape(-1, 1, 2)\n",
    "    transformed_corners = cv2.perspectiveTransform(corners, homography)\n",
    "    img2_rectangle = cv2.polylines(img2, [np.int32(transformed_corners)], True, 255, 3, cv2.LINE_AA)\n",
    "    \n",
    "    output_path = os.path.join(output_, f'frame_sift_{count}.png')\n",
    "    cv2.imwrite(output_path, frame_with_keypoints)\n",
    "    \n",
    "    #cv2.imshow('Object Detection', img2_rectangle)\n",
    "\n",
    "    img1 = img2\n",
    "    gray1 = gray2\n",
    "    keypoints1 = keypoints2\n",
    "    descriptors1 = descriptors2\n",
    "\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8600a5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "cap = cv2.VideoCapture('myVideo6.mp4')\n",
    "count = 0\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "output_ = \"C:\\\\Users\\\\punee\\\\AI-ML\\\\Deep_learning_Exercises\\\\output_Sift\"\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    count += 1\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    keypoints, descriptors = sift.detectAndCompute(gray_frame, None)\n",
    "\n",
    "    frame_with_keypoints = cv2.drawKeypoints(gray_frame, keypoints, None)\n",
    "    output_path = os.path.join(output_, f'frame_sift_{count}.png')\n",
    "    cv2.imwrite(output_path, frame_with_keypoints)\n",
    "    #cv2.imshow(f'Video Frame {count}', frame_with_keypoints)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b236db88",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(image\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m], np\u001b[38;5;241m.\u001b[39muint8)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Define the rectangle for initial GrabCut algorithm\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv2' is not defined"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('group.jpeg')\n",
    "mask = np.zeros(image.shape[:2], np.uint8)\n",
    "# Define the rectangle for initial GrabCut algorithm\n",
    "rect = (50, 50, 450, 290)\n",
    "# Initialize the foreground and background models\n",
    "bgd_model = np.zeros((1, 65), np.float64)\n",
    "fgd_model = np.zeros((1, 65), np.float64)\n",
    "# Apply GrabCut algorithm\n",
    "cv2.grabCut(image, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)\n",
    "# Modify the mask to create a binary mask for foreground and background\n",
    "mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "# Multiply the original image with the binary mask to extract the foreground\n",
    "result = image * mask2[:, :, np.newaxis]\n",
    "cv2.imshow('Original Image', image)\n",
    "cv2.imshow('Foreground Extraction', result)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ab14196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hurray\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "output_ = \"C:\\\\Users\\\\punee\\\\AI-ML\\\\Deep_learning_Exercises\\\\outputGrabcut\"\n",
    "vid = cv2.VideoCapture('myVideo7.mp4')\n",
    "\n",
    "while True:\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    mask = np.zeros(frame.shape[:2], np.uint8)\n",
    "    rect = (50, 50, 450, 290)\n",
    "    bgd_model = np.zeros((1, 65), np.float64)\n",
    "    fgd_model = np.zeros((1, 65), np.float64)\n",
    "    cv2.grabCut(frame, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)\n",
    "    mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "    result = frame * mask2[:, :, np.newaxis]\n",
    "    #cv2.imshow('Original Frame', frame)\n",
    "    #cv2.imshow('Foreground Extraction', result)\n",
    "    frameNo = int(vid.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "    output = os.path.join(output_, f'frame_{frameNo}.png')\n",
    "    cv2.imwrite(output, result)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hurray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5c8a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "output_ = \"C:\\\\Users\\\\punee\\\\AI-ML\\\\Deep_learning_Exercises\\\\outputGrabcut\"\n",
    "vid = cv2.VideoCapture('myVideo7.mp4')\n",
    "face = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    \n",
    "    mask = np.zeros(frame.shape[:2], np.uint8)\n",
    "    rect = (50, 50, 450, 290)\n",
    "    \n",
    "    bgd_model = np.zeros((1, 65), np.float64)\n",
    "    fgd_model = np.zeros((1, 65), np.float64)\n",
    "    cv2.grabCut(frame, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)\n",
    "    mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "    result = frame * mask2[:, :, np.newaxis]\n",
    "    #cv2.imshow('Original Frame', frame)\n",
    "    #cv2.imshow('Foreground Extraction', result)\n",
    "    \n",
    "    gray_img = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face.detectMultiScale(gray_img, scaleFactor=1.3, minNeighbors=5)\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(result, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    #cv2.imshow('Detected Faces', img)\n",
    "    \n",
    "    frameNo = int(vid.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "    output = os.path.join(output_, f'frame_{frameNo}.png')\n",
    "    cv2.imwrite(output, frame)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hurray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50282fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50821b37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54fdf303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hurray\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "output_folder = \"C:\\\\Users\\\\punee\\\\AI-ML\\\\Deep_learning_Exercises\\\\outputGrabcut\"\n",
    "vid = cv2.VideoCapture('myVideo7.mp4')\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    mask = np.zeros(frame.shape[:2], np.uint8)\n",
    "    rect = (50, 50, 450, 290)\n",
    "\n",
    "    bgd_model = np.zeros((1, 65), np.float64)\n",
    "    fgd_model = np.zeros((1, 65), np.float64)\n",
    "    cv2.grabCut(frame, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)\n",
    "    mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "    result = frame * mask2[:, :, np.newaxis]\n",
    "\n",
    "    gray_img = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_img, scaleFactor=1.3, minNeighbors=5)\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(result, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "    frame_no = int(vid.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "    output_path = os.path.join(output_folder, f'frame_{frame_no}.png')\n",
    "    cv2.imwrite(output_path, result)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hurray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af44d06",
   "metadata": {},
   "source": [
    "# detect the faces in the video and then apply grabcut algo to detect faces and focus faces.. it can be used in cctv face detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c391134",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidImage",
     "evalue": "Image not valid.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidImage\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m vid \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmyVideo7.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m face_cascade \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mdetect_faces(vid)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     13\u001b[0m     ret, frame \u001b[38;5;241m=\u001b[39m vid\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\mtcnn\\mtcnn.py:285\u001b[0m, in \u001b[0;36mMTCNN.detect_faces\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;124;03mDetects bounding boxes from the specified image.\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;124;03m:param img: image to process\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m:return: list containing all the bounding boxes detected with their keypoints.\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(img, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidImage(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage not valid.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m height, width, _ \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    288\u001b[0m stage_status \u001b[38;5;241m=\u001b[39m StageStatus(width\u001b[38;5;241m=\u001b[39mwidth, height\u001b[38;5;241m=\u001b[39mheight)\n",
      "\u001b[1;31mInvalidImage\u001b[0m: Image not valid."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "output_folder = \"C:\\\\Users\\\\punee\\\\AI-ML\\\\Deep_learning_Exercises\\\\outputGrabcut\"\n",
    "vid = cv2.VideoCapture('myVideo7.mp4')\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "while True:\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray_img = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_img, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "        mask = np.zeros(frame.shape[:2], np.uint8)\n",
    "        rect = (50, 50, 450, 290)\n",
    "\n",
    "        bgd_model = np.zeros((1, 65), np.float64)\n",
    "        fgd_model = np.zeros((1, 65), np.float64)\n",
    "        cv2.grabCut(frame, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)\n",
    "        mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "        result = frame * mask2[:, :, np.newaxis]\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(result, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "        frame_no = int(vid.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        output_path = os.path.join(output_folder, f'frame_{frame_no}.png')\n",
    "        cv2.imwrite(output_path, result)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hurray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "382ed55b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1316040859.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 11\u001b[1;36m\u001b[0m\n\u001b[1;33m    if not >=_:\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "vid=cv2.VideoCapture('myVideo6.mp4')\n",
    "inp=float(input(\"Enter the Time where you want to start\"))\n",
    "fps=vid.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "count=0\n",
    "while (vid.isOpened()):\n",
    "    _,frame=vid.read()\n",
    "    if not >=_:\n",
    "        break\n",
    "    count+=1\n",
    "    if count >= (fps*inp):\n",
    "        frame=cv2.resize(frame,(640,640),interpolation=cv2.INTER_CUBIC)\n",
    "        cv2.imshow(f\"Frames From {inp} Seconds-{count}\",frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198d7ddf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b0ceb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83a33fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4171f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 274ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 265ms/step\n",
      "WARNING:tensorflow:5 out of the last 336 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002A24E8B72E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 329ms/step\n",
      "[{'box': [74, 0, 121, 151], 'confidence': 0.9970970153808594, 'keypoints': {'left_eye': (105, 55), 'right_eye': (163, 55), 'nose': (134, 82), 'mouth_left': (111, 116), 'mouth_right': (158, 116)}}]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "detector = MTCNN()\n",
    "img = cv2.imread('eye.jpeg')\n",
    "\n",
    "faces = detector.detect_faces(img)\n",
    "print(faces)\n",
    "for face in faces:\n",
    "    x, y, width, height = face['box']\n",
    "    cv2.rectangle(img, (x, y), (x + width, y + height), (0, 255, 0), 2)\n",
    "cv2.imshow('Face Detection', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fa1391",
   "metadata": {},
   "source": [
    "# face detection using MTCNN Model for better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7078a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 305ms/step\n",
      "WARNING:tensorflow:5 out of the last 20 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000027A78C493A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 280ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 282ms/step\n",
      "1/1 [==============================] - 0s 367ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "2/2 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 97ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 121ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 156ms/step\n",
      "1/1 [==============================] - 0s 116ms/step\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "2/2 [==============================] - 0s 10ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 104ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 165ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 102ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "2/2 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 92ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 108ms/step\n",
      "1/1 [==============================] - 0s 127ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "hurray\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "detector = MTCNN()\n",
    "\n",
    "output_folder = \"C:\\\\Users\\\\punee\\\\AI-ML\\\\Deep_learning_Exercises\\\\outputGrabcut\"\n",
    "vid = cv2.VideoCapture('myVideo4.mp4')\n",
    "\n",
    "while True:\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    faces = detector.detect_faces(frame)\n",
    "\n",
    "    for face in faces:\n",
    "        x, y, width, height = face['box']\n",
    "        cv2.rectangle(frame, (x, y), (x + width, y + height), (0, 255, 0), 2)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "\n",
    "        frame_no = int(vid.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        output_path = os.path.join(output_folder, f'frame_{frame_no}.png')\n",
    "        cv2.imwrite(output_path, frame)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hurray\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007fb9ba",
   "metadata": {},
   "source": [
    "# face detection after grabcut using mtcnn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae7478f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 260ms/step\n",
      "1/1 [==============================] - 0s 237ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 224ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 126ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 109ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 139ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "hurray\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "detector = MTCNN()\n",
    "\n",
    "output_folder = \"C:\\\\Users\\\\punee\\\\AI-ML\\\\Deep_learning_Exercises\\\\outputGrabcut\"\n",
    "vid = cv2.VideoCapture('myVideo6.mp4')\n",
    "\n",
    "while True:\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    faces = detector.detect_faces(frame)\n",
    "\n",
    "    for face in faces:\n",
    "        x, y, width, height = face['box']\n",
    "        cv2.rectangle(frame, (x, y), (x + width, y + height), (0, 255, 0), 2)\n",
    "\n",
    "    if len(faces) > 0:\n",
    "\n",
    "        mask = np.zeros(frame.shape[:2], np.uint8)\n",
    "        rect = (50, 50, 450, 290)\n",
    "\n",
    "        bgd_model = np.zeros((1, 65), np.float64)\n",
    "        fgd_model = np.zeros((1, 65), np.float64)\n",
    "        cv2.grabCut(frame, mask, rect, bgd_model, fgd_model, 5, cv2.GC_INIT_WITH_RECT)\n",
    "        mask2 = np.where((mask == 2) | (mask == 0), 0, 1).astype('uint8')\n",
    "        result = frame * mask2[:, :, np.newaxis]\n",
    "\n",
    "        frame_no = int(vid.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "        print(frame_no)\n",
    "        output_path = os.path.join(output_folder, f'Frame_{frame_no}.png')\n",
    "        print(output_path)\n",
    "        cv2.imwrite(output_path, result)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "vid.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"hurray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded500f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e2b471b4",
   "metadata": {},
   "source": [
    "# face detection by taking fases points of eye, mouth, nose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd04f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 228ms/step\n",
      "1/1 [==============================] - 0s 167ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 283ms/step\n",
      "1/1 [==============================] - 0s 218ms/step\n",
      "[{'box': [177, 26, 42, 55], 'confidence': 0.9784212112426758, 'keypoints': {'left_eye': (189, 47), 'right_eye': (209, 48), 'nose': (198, 63), 'mouth_left': (190, 71), 'mouth_right': (205, 71)}}]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "detector = MTCNN()\n",
    "img = cv2.imread('maskk.jpeg')\n",
    "faces = detector.detect_faces(img)\n",
    "print(faces)\n",
    "for face in faces:\n",
    "    x, y, width, height = face['box']\n",
    "    cv2.rectangle(img, (x, y), (x + width, y + height), (0, 255, 0), 4)\n",
    "    points = face['keypoints']\n",
    "    eyeLeft = (points['left_eye'][0] - 10, points['left_eye'][1] - 10, 20, 20)\n",
    "    eyeRight = (points['right_eye'][0] - 10, points['right_eye'][1] - 10, 20, 20)\n",
    "    nose = (points['nose'][0] - 15, points['nose'][1] - 15, 30, 30)\n",
    "    mouth_left = (points['mouth_left'][0] - 15, points['mouth_left'][1] - 10, 30, 20)\n",
    "    mouth_right = (points['mouth_right'][0] - 15, points['mouth_right'][1] - 10, 30, 20)\n",
    "\n",
    "    cv2.rectangle(img, eyeLeft, (255, 0, 0), 2)\n",
    "    cv2.rectangle(img, eyeRight, (255, 0, 0), 2)\n",
    "    cv2.rectangle(img, nose, (0, 0, 255), 2)\n",
    "    cv2.rectangle(img, mouth_left, (0, 255, 0), 2)\n",
    "    cv2.rectangle(img, mouth_right, (0, 255, 0), 2)\n",
    "cv2.imshow('Face Detection', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b946bb",
   "metadata": {},
   "source": [
    "# sharpen the image to enhance the quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b3e4008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "img=cv2.imread(\"face.jpeg\")\n",
    "kernel=np.array([[-1,-1,-1],[-1,9,-1],[-1,-1,-1]])\n",
    "outp=cv2.filter2D(img,-1,kernel)\n",
    "finall=cv2.add(img,outp)\n",
    "cv2.imshow(\"Original\",img)\n",
    "cv2.imshow(\"Modified\",outp)\n",
    "cv2.imshow(\"Modified2\",finall)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f646aca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
